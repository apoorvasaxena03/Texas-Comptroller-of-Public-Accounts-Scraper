{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Texas Comptroller of Public Accounts - Scraper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Libraries\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.remote.webdriver import WebDriver\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from io import StringIO\n",
    "\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from typing import Optional, List\n",
    "\n",
    "# Import date class from datetime module\n",
    "from datetime import date\n",
    "\n",
    "import time\n",
    "import os\n",
    "from pathlib import Path  # Import Path to handle file paths easily\n",
    "\n",
    "# Importing Logger\n",
    "from custom_logger import CustomLogger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing Scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[scraper_ipynb] INFO (10-11 12:21 AM): ################## Logging Started ################## (Line: 12) [1293263072.py]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define log directory and ensure it exists\n",
    "log_dir = r\"C:\\Users\\Apoorva.Saxena\\OneDrive - Sitio Royalties\\Desktop\\Project - Apoorva\\Python\\Scraping\\Texas-Comptroller-of-Public-Accounts-Scraper\\logs\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "# Create a CustomLogger instance\n",
    "logger = CustomLogger(log_file_name=\"scraper_log\", log_dir_path=log_dir, logger_name='scraper_ipynb').get_logger()\n",
    "\n",
    "# Start the logger\n",
    "logger.info(f\"################## Logging Started ##################\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Scraper Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class _LeaseDropNaturalGas_WebScraper:\n",
    "\n",
    "    def __init__(self, scraped_csv: str = \"scraped_leases.csv\") -> None:\n",
    "        \"\"\"\n",
    "        Initialize the web scraper.\n",
    "\n",
    "        Args:\n",
    "            scraped_csv (str): The CSV file name or path to store scraped data. Defaults to \"scraped_leases.csv\".\n",
    "        \"\"\"\n",
    "        self.site_key: str = '6Lf6Z5sUAAAAACg7ECAeRMcnAo2_WfoKUeNYXkj_'\n",
    "        self.login_url: str = 'https://mycpa.cpa.state.tx.us/cong/loginForward.do?phase=check'\n",
    "        self.ngl_drop_url: str = 'https://mycpa.cpa.state.tx.us/cong/leaseDropNGAction.do'\n",
    "        self.xpath_leaseNo: str = '//*[@id=\"leaseNum\"]'\n",
    "        self.xpath_begDt: str = '//*[@id=\"begFilPrd\"]'\n",
    "        self.xpath_endDt: str = '//*[@id=\"endFilPrd\"]'\n",
    "        self.xpath_submitForm: str = '//*[@id=\"leaseDropNGForm\"]/span[7]/p/input'\n",
    "        self.xpath_lease_table: str = '//*[@id=\"menucontenttable\"]/table/tbody/tr/td[2]/div/table'\n",
    "        self.driver: WebDriver  = None\n",
    "        self._initialize_driver()\n",
    "\n",
    "        # Handle the file path: if the user provides a relative path, convert it to an absolute path.\n",
    "        self.scraped_csv: str = os.path.abspath(scraped_csv)\n",
    "\n",
    "\n",
    "    def _initialize_driver(self) -> None:\n",
    "        \"\"\"\n",
    "        Initializes the Chrome WebDriver.\n",
    "        \"\"\"\n",
    "        options = webdriver.ChromeOptions()\n",
    "        # options.add_argument('--headless')  # Optional: run in headless mode\n",
    "        options.add_argument('--disable-gpu')  # Optional: disable GPU\n",
    "        options.add_argument('--no-sandbox')  # Optional: required for some environments\n",
    "\n",
    "        self.driver = webdriver.Chrome(options=options)\n",
    "\n",
    "\n",
    "    def _load_page(self) -> None:\n",
    "        \"\"\"\n",
    "        Load the login and Natural Gas Inquiry drop page.\n",
    "        \"\"\"\n",
    "        if self.driver is None:\n",
    "            raise RuntimeError(\"WebDriver is not initialized.\")\n",
    "        \n",
    "        self.driver.maximize_window()\n",
    "        self.driver.get(self.login_url)\n",
    "        time.sleep(0.8)\n",
    "        self.driver.get(self.ngl_drop_url)\n",
    "        wait = WebDriverWait(self.driver, 1)\n",
    "        wait.until(lambda d: d.execute_script(\"return typeof grecaptcha !== 'undefined'\"))\n",
    "\n",
    "\n",
    "    def _get_recaptcha_token(self) -> str:\n",
    "        \"\"\"\n",
    "        Retrieve the reCAPTCHA token from the webpage.\n",
    "\n",
    "        Returns:\n",
    "            Optional[str]: The reCAPTCHA token as a string, or None if the token retrieval fails.\n",
    "        \"\"\"\n",
    "        \n",
    "        if self.driver is None:\n",
    "            raise RuntimeError(\"WebDriver is not initialized.\")\n",
    "\n",
    "        self._load_page()\n",
    "        \n",
    "        token = self.driver.execute_script(f'''\n",
    "            return grecaptcha.execute('{self.site_key}', {{action: 'homepage'}}).then(function(token) {{\n",
    "                return token;\n",
    "            }});\n",
    "        ''')\n",
    "        \n",
    "        return token\n",
    "\n",
    "\n",
    "    def _get_NGL_Inquiry_html(self, lease_no: str, beg_dt: str, end_dt: str) -> str:\n",
    "        \"\"\"\n",
    "        Scrape the Natural Gas Inquiry form based on lease_no, beg_dt, and end_dt.\n",
    "        \n",
    "        Args:\n",
    "            lease_no (str): The lease number to search (6 or all digits).\n",
    "            beg_dt (str): Begining period (yymm or yy)\n",
    "            end_dt (str): Ending period (yymm or yy)\n",
    "        \n",
    "        Returns:\n",
    "            Optional[str]: The HTML content of the page or None if an error occurred.\n",
    "        \"\"\"\n",
    "\n",
    "        if self.driver is None:\n",
    "            raise RuntimeError(\"WebDriver is not initialized.\")\n",
    "        \n",
    "        # Format Lease Number\n",
    "        try:\n",
    "            if len(lease_no) == 11:\n",
    "                formatted_lease_no = lease_no.split('-')[1]\n",
    "            \n",
    "            elif len(lease_no) == 6:\n",
    "                formatted_lease_no = lease_no\n",
    "\n",
    "        except ValueError:\n",
    "            print(f'Lease number entered not of 6 or 11 digits')\n",
    "        \n",
    "        # Loading the page\n",
    "        self._load_page()\n",
    "\n",
    "        # Filling Lease Number\n",
    "        self.driver.find_element(By.XPATH, self.xpath_leaseNo).send_keys(formatted_lease_no)\n",
    "\n",
    "        # Filling Begining Period\n",
    "        self.driver.find_element(By.XPATH, self.xpath_begDt).send_keys(beg_dt)\n",
    "\n",
    "        # Filling Ending Period\n",
    "        self.driver.find_element(By.XPATH, self.xpath_endDt).send_keys(end_dt)\n",
    "\n",
    "        # Running the Inquiry Form\n",
    "        time.sleep(0.8)\n",
    "        self.driver.find_element(By.XPATH, self.xpath_submitForm).click()\n",
    "\n",
    "        # Visibility of the table header element\n",
    "        try:\n",
    "            # Wait until the table is located or timeout occurs\n",
    "            lease_table = WebDriverWait(self.driver, 10).until(\n",
    "                EC.presence_of_all_elements_located(\n",
    "                    (By.XPATH, self.xpath_lease_table)\n",
    "                )\n",
    "            )\n",
    "\n",
    "            if lease_table:\n",
    "                return self.driver.page_source\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Lease table not found for {lease_no}\")\n",
    "            return False\n",
    "    \n",
    "\n",
    "    def _clear_entry_labels(self) -> None:\n",
    "        \"\"\"\n",
    "        Clear the input fields for Lease Number, Beginning Period, and Ending Period.\n",
    "\n",
    "        Returns:\n",
    "            None: This function does not return anything.\n",
    "        \"\"\"\n",
    "        # Clearing Lease Number\n",
    "        self.driver.find_element(By.XPATH, self.xpath_leaseNo).clear()\n",
    "\n",
    "        # Clearing Begining Period\n",
    "        self.driver.find_element(By.XPATH, self.xpath_begDt).clear()\n",
    "\n",
    "        # Clearing Ending Period\n",
    "        self.driver.find_element(By.XPATH, self.xpath_endDt).clear()\n",
    "\n",
    "\n",
    "    def _parse_html(self, html: str, raw: bool = False) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Parsing HTML content using Beautiful Soup into a DataFrame\n",
    "        \n",
    "        Args:\n",
    "            html (str): The HTML content as string.\n",
    "        \n",
    "        Returns:\n",
    "            pd.DataFrame: The parsed and cleaned DataFrame.\n",
    "        \"\"\"\n",
    "        soup = BeautifulSoup(html,'html.parser')\n",
    "\n",
    "        df_raw = pd.read_html(StringIO(str(soup.find_all('table'))))\n",
    "\n",
    "        df_LeaseNGL_raw = df_raw[1]\n",
    "\n",
    "        # Step 1: Identify rows where 'Primary Taxpayer #' contains 'Period' and extract the date part\n",
    "        df_LeaseNGL_raw['prod_dt'] = np.where(\n",
    "            df_LeaseNGL_raw['Primary Taxpayer #'].str.contains('Period', na=False),\n",
    "            df_LeaseNGL_raw['Primary Taxpayer #'].str.extract(r'Period: (\\d{4})', expand=False),\n",
    "            np.nan\n",
    "        )\n",
    "\n",
    "        # Step 2: Forward fill the 'prod_dt' column to propagate the last valid date value\n",
    "        df_LeaseNGL_raw['prod_dt'] = df_LeaseNGL_raw['prod_dt'].ffill()\n",
    "\n",
    "\n",
    "        # Step 3: Convert 'prod_dt' from 'YYMM' to datetime format 'YYYY-MM-DD'\n",
    "        df_LeaseNGL_raw['prod_dt'] = pd.to_datetime(df_LeaseNGL_raw['prod_dt'], format='%y%m')\n",
    "\n",
    "        df_LeaseNGL_raw.insert(0, 'prod_dt', df_LeaseNGL_raw.pop('prod_dt')) # Insert 'prod_dt' as the first column\n",
    "\n",
    "        # Step 4: Filter out rows where column 'Primary Taxpayer #' contains 'Period'\n",
    "        df_LeaseNGL_cleaned = df_LeaseNGL_raw[~df_LeaseNGL_raw['Primary Taxpayer #'].str.contains('Period', na=False)].reset_index(drop=True)\n",
    "\n",
    "        # Step 5: Clean column names\n",
    "        df_LeaseNGL_cleaned.columns = df_LeaseNGL_cleaned.columns.str.lower()  # Convert to lowercase\n",
    "        df_LeaseNGL_cleaned.columns = df_LeaseNGL_cleaned.columns.str.replace('#', '')  # Remove '#' character\n",
    "        df_LeaseNGL_cleaned.columns = df_LeaseNGL_cleaned.columns.str.replace(' ', '_')  # Replace spaces with underscores\n",
    "\n",
    "        if raw:\n",
    "            return df_raw, df_LeaseNGL_cleaned\n",
    "        else:\n",
    "            return df_LeaseNGL_cleaned\n",
    "\n",
    "\n",
    "    def _read_scraped_csv(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Read the CSV file containing previously scraped leases.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: The DataFrame containing already scraped lease information.\n",
    "        \"\"\"\n",
    "        cols = ['lease_number', 'prod_dt', 'sub_type', 'primary_taxpayer_', 'comm_code', 'lse_typ',\n",
    "                'cnty/_dpi', 'exmt_typ', 'api_nbr', 'off_lease', 'other_party_taxpayer',\n",
    "                'secondary_tp_name', 'tax_reimb', 'ttl_lease_volume', 'your_volume',\n",
    "                'your_value', 'tax_due', 'gr_volume', 'gr_value', 'marketing_cost',\n",
    "                'net_tax_value', 'tax_rate', '05_tax_due', 'error_status'\n",
    "                ]\n",
    "        \n",
    "\n",
    "        if os.path.exists(self.scraped_csv):\n",
    "            return pd.read_csv(self.scraped_csv)\n",
    "        return pd.DataFrame(columns=cols)\n",
    "    \n",
    "\n",
    "    def _append_to_csv(self, df: pd.DataFrame) -> None:\n",
    "        \"\"\"\n",
    "        Append the newly scraped lease data to the CSV file.\n",
    "\n",
    "        Args:\n",
    "            df (pd.DataFrame): The DataFrame with new lease data.\n",
    "        \"\"\"\n",
    "        if os.path.exists(self.scraped_csv):\n",
    "            df.to_csv(self.scraped_csv, mode='a', header=False, index=False)\n",
    "        else:\n",
    "            df.to_csv(self.scraped_csv, index=False)\n",
    "\n",
    "\n",
    "    def _quit(self) -> None:\n",
    "        \"\"\"\n",
    "        Close and quit the WebDriver.\n",
    "        \"\"\"\n",
    "        if self.driver is not None:\n",
    "            self.driver.close()\n",
    "            self.driver.quit()\n",
    "            self.driver = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to handle scraping outside of the class\n",
    "def scrape_leases(scraper: _LeaseDropNaturalGas_WebScraper, leases_df: pd.DataFrame, limit: int = 1000) -> None:\n",
    "    \"\"\"\n",
    "    Scrape leases from a DataFrame and stop after scraping 1,000 rows. Pause after every 20 rows.\n",
    "\n",
    "    Args:\n",
    "        scraper (_LeaseDropNaturalGas_WebScraper): The initialized scraper instance.\n",
    "        leases_df (pd.DataFrame): The DataFrame containing lease_number, beg_dt, and end_dt columns.\n",
    "        limit (int): The maximum number of leases to scrape in one session. Defaults to 1,000.\n",
    "    \"\"\"\n",
    "    scraped_df = scraper._read_scraped_csv()\n",
    "\n",
    "    # Filter out leases that have already been scraped\n",
    "    leases_to_scrape = leases_df[~(leases_df['lease_number'].isin(scraped_df['lease_number'].unique()))]\n",
    "\n",
    "\n",
    "    scraped_data: List[pd.DataFrame] = []\n",
    "    count: int = 0\n",
    "\n",
    "    for _, row in leases_to_scrape.iterrows():\n",
    "        lease_no = row['lease_number']\n",
    "\n",
    "        html_content: Optional[str] = scraper._get_NGL_Inquiry_html(lease_no, beg_dt='2201', end_dt='2410')\n",
    "        if html_content:\n",
    "            df: pd.DataFrame = scraper._parse_html(html_content)\n",
    "            df['lease_number'] = lease_no\n",
    "            scraped_data.append(df)\n",
    "\n",
    "        count += 1\n",
    "        if count % 20 == 0:\n",
    "            print(f\"Pausing for 30 seconds after scraping {count} leases.\")\n",
    "            time.sleep(30)\n",
    "\n",
    "        if count >= limit:\n",
    "            break\n",
    "\n",
    "    # Save scraped data to CSV\n",
    "    if scraped_data:\n",
    "        full_df = pd.concat(scraped_data, ignore_index=True)\n",
    "        scraper._append_to_csv(full_df)\n",
    "        print(f\"Scraped {count} leases and saved to {scraper.scraped_csv}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2409'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(date.today().year)[2:] + str(date.today().month).zfill(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the scraper\n",
    "\n",
    "scraper = _LeaseDropNaturalGas_WebScraper()\n",
    "\n",
    "try:\n",
    "    # Fill the form and get the HTML content\n",
    "    html_content = scraper._get_NGL_Inquiry_html(lease_no='7C-017147-O', beg_dt='2101', end_dt='2410')\n",
    "\n",
    "    # Parse the HTML and get the cleaned DataFrame\n",
    "    if html_content:\n",
    "        df = scraper._parse_html(html=html_content)\n",
    "finally:\n",
    "    scraper._quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Well Header Data from CC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading well header csv to pandas DataFrame\n",
    "df_wellheader_raw = pd.read_csv('well_header.csv',low_memory=False)\n",
    "\n",
    "# Cleaning up the column names\n",
    "df_wellheader_modified = df_wellheader_raw.copy() # Creating\n",
    "df_wellheader_modified.columns = df_wellheader_modified.columns.str.lower().str.replace(' ', '_')  # Convert to lowercase and replace spaces with underscores\n",
    "\n",
    "# Convert 'first_prod_date' column to datetime\n",
    "df_wellheader_modified['first_prod_date'] = pd.to_datetime(df_wellheader_modified['first_prod_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([11,  0,  5])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the length of the values in lease_number columns\n",
    "df_wellheader_modified['lease_number'].apply(lambda x: len(str(x)) if pd.notnull(x) else 0).unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only grab Lease Nos. that are complete\n",
    "df_wellHeader_complete_LeaseNo = df_wellheader_modified[df_wellheader_modified['lease_number'].apply(lambda x: len(str(x)) if pd.notnull(x) else 0) == 11].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17506, 303)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_wellHeader_complete_LeaseNo.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatting_WellHeader_CC():\n",
    "    \n",
    "    # Reading well header csv to pandas DataFrame\n",
    "    df_wellheader_raw = pd.read_csv('well_header.csv',low_memory=False)\n",
    "\n",
    "    # Cleaning up the column names\n",
    "    df_wellheader_modified = df_wellheader_raw.copy() # Copying the DataFrame\n",
    "    df_wellheader_modified.columns = df_wellheader_modified.columns.str.lower().str.replace(' ', '_')  # Convert to lowercase and replace spaces with underscores\n",
    "\n",
    "    # Only grab Lease Nos. that are complete\n",
    "    df_wellHeader_complete_LeaseNo = df_wellheader_modified[df_wellheader_modified['lease_number'].apply(lambda x: len(str(x)) if pd.notnull(x) else 0) == 11].reset_index(drop=True)\n",
    "\n",
    "    # Returning unique LeaseNo. from well header\n",
    "    return pd.DataFrame(df_wellHeader_complete_LeaseNo['lease_number'].unique(), columns=['lease_number'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
