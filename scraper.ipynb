{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Texas Comptroller of Public Accounts - Scraper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Libraries\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.remote.webdriver import WebDriver\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException, NoSuchElementException\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from io import StringIO\n",
    "\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from typing import Optional, List\n",
    "\n",
    "# Import date class from datetime module\n",
    "from datetime import date\n",
    "\n",
    "import time\n",
    "import os\n",
    "from pathlib import Path  # Import Path to handle file paths easily\n",
    "\n",
    "# Importing Logger\n",
    "from custom_logger import CustomLogger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing Scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[scraper_ipynb] INFO (10-13 04:28 PM): ################## Logging Started ################## (Line: 11) [772446353.py]\n"
     ]
    }
   ],
   "source": [
    "# Define log directory and ensure it exists\n",
    "log_dir = r\"C:\\Users\\Apoorva.Saxena\\OneDrive - Sitio Royalties\\Desktop\\Project - Apoorva\\Python\\Scraping\\Texas-Comptroller-of-Public-Accounts-Scraper\\logs\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "# Create a CustomLogger instance\n",
    "logger = CustomLogger(log_file_name=\"scraper_log\",\n",
    "                      log_dir_path=log_dir, logger_name='scraper_ipynb'\n",
    "                      ).get_logger()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Scraper Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class _LeaseDropNaturalGas_WebScraper:\n",
    "\n",
    "    def __init__(self, csv_dir: str = \"./\", scraped_csv: str = \"scraped_leases.csv\") -> None:\n",
    "        \"\"\"\n",
    "        Initialize the web scraper.\n",
    "\n",
    "        Args:\n",
    "            scraped_csv (str): The CSV file name or path to store scraped data. Defaults to \"scraped_leases.csv\".\n",
    "        \n",
    "        Example:\n",
    "            scraper = _LeaseDropNaturalGas_WebScraper(csv_dir=\"/path/to/directory\")\n",
    "        \"\"\"\n",
    "        self.site_key: str = '6Lf6Z5sUAAAAACg7ECAeRMcnAo2_WfoKUeNYXkj_'\n",
    "        self.login_url: str = 'https://mycpa.cpa.state.tx.us/cong/loginForward.do?phase=check'\n",
    "        self.ngl_drop_url: str = 'https://mycpa.cpa.state.tx.us/cong/leaseDropNGAction.do'\n",
    "        self.xpath_leaseNo: str = '//*[@id=\"leaseNum\"]'\n",
    "        self.xpath_begDt: str = '//*[@id=\"begFilPrd\"]'\n",
    "        self.xpath_endDt: str = '//*[@id=\"endFilPrd\"]'\n",
    "        self.xpath_submitForm: str = '//*[@id=\"leaseDropNGForm\"]/span[7]/p/input'\n",
    "        self.xpath_lease_table: str = '//*[@id=\"menucontenttable\"]/table/tbody/tr/td[2]/div/table'\n",
    "        self.xpath_error: str = '//*[@id=\"leaseDropNGForm\"]/span[2]/ul/li/strong'\n",
    "        self.driver: WebDriver  = None\n",
    "        self._initialize_driver()\n",
    "\n",
    "        # Handle the file path: set the absolute path for the CSV file\n",
    "        self.scraped_csv: str = os.path.join(os.path.abspath(csv_dir), scraped_csv)\n",
    "\n",
    "\n",
    "    def _initialize_driver(self) -> None:\n",
    "        \"\"\"\n",
    "        Initializes the Chrome WebDriver.\n",
    "        \"\"\"\n",
    "        options = webdriver.ChromeOptions()\n",
    "        # options.add_argument('--headless')  # Optional: run in headless mode\n",
    "        options.add_argument('--disable-gpu')  # Optional: disable GPU\n",
    "        options.add_argument('--no-sandbox')  # Optional: required for some environments\n",
    "\n",
    "        self.driver = webdriver.Chrome(options=options)\n",
    "\n",
    "\n",
    "    def _load_page(self) -> None:\n",
    "        \"\"\"\n",
    "        Load the login and Natural Gas Inquiry drop page.\n",
    "        \"\"\"\n",
    "        if self.driver is None:\n",
    "            raise RuntimeError(\"WebDriver is not initialized.\")\n",
    "        \n",
    "        self.driver.maximize_window()\n",
    "        self.driver.get(self.login_url)\n",
    "        time.sleep(0.5)\n",
    "        self.driver.get(self.ngl_drop_url)\n",
    "        wait = WebDriverWait(self.driver, 3)\n",
    "        wait.until(lambda d: d.execute_script(\"return typeof grecaptcha !== 'undefined'\"))\n",
    "\n",
    "\n",
    "    def _get_recaptcha_token(self) -> str:\n",
    "        \"\"\"\n",
    "        Retrieve the reCAPTCHA token from the webpage.\n",
    "\n",
    "        Returns:\n",
    "            Optional[str]: The reCAPTCHA token as a string, or None if the token retrieval fails.\n",
    "        \"\"\"\n",
    "        \n",
    "        if self.driver is None:\n",
    "            raise RuntimeError(\"WebDriver is not initialized.\")\n",
    "\n",
    "        self._load_page()\n",
    "        \n",
    "        token = self.driver.execute_script(f'''\n",
    "            return grecaptcha.execute('{self.site_key}', {{action: 'homepage'}}).then(function(token) {{\n",
    "                return token;\n",
    "            }});\n",
    "        ''')\n",
    "        \n",
    "        return token\n",
    "\n",
    "\n",
    "    def _get_NGL_Inquiry_html(self, lease_no: str, beg_dt: str, end_dt: str, max_retries: int = 3) -> Optional[str]:\n",
    "        \"\"\"\n",
    "        Scrape the Natural Gas Inquiry form based on lease_no, beg_dt, and end_dt.\n",
    "        \n",
    "        Args:\n",
    "            lease_no (str): The lease number to search (6 or all digits).\n",
    "            beg_dt (str): Beginning period (yymm or yy).\n",
    "            end_dt (str): Ending period (yymm or yy).\n",
    "            max_retries (int): Maximum number of retries if the form submission fails. Defaults to 3.\n",
    "        \n",
    "        Returns:\n",
    "            Optional[str]: The HTML content of the page or None if an error occurred.\n",
    "        \"\"\"\n",
    "        if self.driver is None:\n",
    "            raise RuntimeError(\"WebDriver is not initialized.\")\n",
    "        \n",
    "        # Format Lease Number\n",
    "        try:\n",
    "            if len(lease_no) == 11:\n",
    "                formatted_lease_no = lease_no.split('-')[1]\n",
    "            elif len(lease_no) == 6:\n",
    "                formatted_lease_no = lease_no\n",
    "        except ValueError as e:\n",
    "            logger.error(f'Lease number {lease_no} is not of 6 or 11 digits:', e)\n",
    "            return None\n",
    "        \n",
    "        # Retry loop for submitting the form and handling errors\n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                # Fill in the lease number\n",
    "                self.driver.find_element(By.XPATH, self.xpath_leaseNo).send_keys(formatted_lease_no)\n",
    "                # Fill in the beginning and ending periods\n",
    "                self.driver.find_element(By.XPATH, self.xpath_begDt).send_keys(beg_dt)\n",
    "                self.driver.find_element(By.XPATH, self.xpath_endDt).send_keys(end_dt)\n",
    "\n",
    "                # Submit the form\n",
    "                time.sleep(2)  # Delay before submitting\n",
    "                self.driver.find_element(By.XPATH, self.xpath_submitForm).click()\n",
    "\n",
    "                # Check for the _error object to validate the success of the submission\n",
    "                try:\n",
    "                    _error = WebDriverWait(self.driver, 3).until(\n",
    "                        EC.presence_of_element_located((By.XPATH, self.xpath_error))\n",
    "                    )\n",
    "                    if _error:\n",
    "                        logger.warning(f\"Error detected for lease {lease_no} on attempt {attempt + 1}. Retrying...\")\n",
    "                        time.sleep(2)  # Delay before retrying\n",
    "                        continue  # Retry submission\n",
    "                except TimeoutException:\n",
    "                    pass  # No error found, proceed to check the table\n",
    "\n",
    "                # Wait for the lease table to load or raise a timeout exception\n",
    "                lease_table = WebDriverWait(self.driver, 5).until(\n",
    "                    EC.presence_of_all_elements_located((By.XPATH, self.xpath_lease_table))\n",
    "                )\n",
    "                \n",
    "                # If the lease table is found, return the page source\n",
    "                if lease_table:\n",
    "                    logger.info(f\"Successfully scraped lease {lease_no} (Attempt {attempt + 1})\")\n",
    "                    return self.driver.page_source\n",
    "\n",
    "            except TimeoutException:\n",
    "                logger.warning(f\"Lease table not found for lease {lease_no} on attempt {attempt + 1}. Retrying...\")\n",
    "                time.sleep(1)\n",
    "\n",
    "            except NoSuchElementException as e:\n",
    "                logger.error(f\"NoSuchElementException occurred for lease {lease_no} (Attempt {attempt + 1}): {e}\")\n",
    "                return False\n",
    "        \n",
    "        # If max_retries are exhausted without success\n",
    "        logger.error(f\"Failed to scrape lease {lease_no} after {max_retries} attempts.\")\n",
    "        return False\n",
    "        \n",
    "\n",
    "    def _clear_entry_labels(self) -> None:\n",
    "        \"\"\"\n",
    "        Clear the input fields for Lease Number, Beginning Period, and Ending Period.\n",
    "\n",
    "        Returns:\n",
    "            None: This function does not return anything.\n",
    "        \"\"\"\n",
    "        # Clearing Lease Number\n",
    "        self.driver.find_element(By.XPATH, self.xpath_leaseNo).clear()\n",
    "\n",
    "        # Clearing Begining Period\n",
    "        self.driver.find_element(By.XPATH, self.xpath_begDt).clear()\n",
    "\n",
    "        # Clearing Ending Period\n",
    "        self.driver.find_element(By.XPATH, self.xpath_endDt).clear()\n",
    "\n",
    "\n",
    "    def _parse_html(self, html: str, raw: bool = False) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Parsing HTML content using Beautiful Soup into a DataFrame\n",
    "        \n",
    "        Args:\n",
    "            html (str): The HTML content as string.\n",
    "        \n",
    "        Returns:\n",
    "            pd.DataFrame: The parsed and cleaned DataFrame.\n",
    "        \"\"\"\n",
    "        soup = BeautifulSoup(html,'html.parser')\n",
    "\n",
    "        df_raw = pd.read_html(StringIO(str(soup.find_all('table'))))\n",
    "\n",
    "        df_LeaseNGL_raw = df_raw[1]\n",
    "\n",
    "        # Step 1: Identify rows where 'Primary Taxpayer #' contains 'Period' and extract the date part\n",
    "        df_LeaseNGL_raw['prod_dt'] = np.where(\n",
    "            df_LeaseNGL_raw['Primary Taxpayer #'].str.contains('Period', na=False),\n",
    "            df_LeaseNGL_raw['Primary Taxpayer #'].str.extract(r'Period: (\\d{4})', expand=False),\n",
    "            np.nan\n",
    "        )\n",
    "\n",
    "        # Step 2: Forward fill the 'prod_dt' column to propagate the last valid date value\n",
    "        df_LeaseNGL_raw['prod_dt'] = df_LeaseNGL_raw['prod_dt'].ffill()\n",
    "\n",
    "\n",
    "        # Step 3: Convert 'prod_dt' from 'YYMM' to datetime format 'YYYY-MM-DD'\n",
    "        df_LeaseNGL_raw['prod_dt'] = pd.to_datetime(df_LeaseNGL_raw['prod_dt'], format='%y%m')\n",
    "\n",
    "        df_LeaseNGL_raw.insert(0, 'prod_dt', df_LeaseNGL_raw.pop('prod_dt')) # Insert 'prod_dt' as the first column\n",
    "\n",
    "        # Step 4: Filter out rows where column 'Primary Taxpayer #' contains 'Period'\n",
    "        df_LeaseNGL_cleaned = df_LeaseNGL_raw[~df_LeaseNGL_raw['Primary Taxpayer #'].str.contains('Period', na=False)].reset_index(drop=True)\n",
    "\n",
    "        # Step 5: Clean column names\n",
    "        df_LeaseNGL_cleaned.columns = df_LeaseNGL_cleaned.columns.str.lower()  # Convert to lowercase\n",
    "        df_LeaseNGL_cleaned.columns = df_LeaseNGL_cleaned.columns.str.replace('#', '')  # Remove '#' character\n",
    "        df_LeaseNGL_cleaned.columns = df_LeaseNGL_cleaned.columns.str.replace(' ', '_')  # Replace spaces with underscores\n",
    "\n",
    "        if raw:\n",
    "            return df_raw, df_LeaseNGL_cleaned\n",
    "        else:\n",
    "            return df_LeaseNGL_cleaned\n",
    "\n",
    "\n",
    "    def _read_scraped_csv(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Read the CSV file containing previously scraped leases.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: The DataFrame containing already scraped lease information.\n",
    "        \"\"\"\n",
    "        cols = ['lease_number', 'prod_dt', 'sub_type', 'primary_taxpayer_', 'comm_code', 'lse_typ',\n",
    "                'cnty/_dpi', 'exmt_typ', 'api_nbr', 'off_lease', 'other_party_taxpayer',\n",
    "                'secondary_tp_name', 'tax_reimb', 'ttl_lease_volume', 'your_volume',\n",
    "                'your_value', 'tax_due', 'gr_volume', 'gr_value', 'marketing_cost',\n",
    "                'net_tax_value', 'tax_rate', '05_tax_due', 'error_status'\n",
    "                ]\n",
    "        \n",
    "\n",
    "        if os.path.exists(self.scraped_csv):\n",
    "            return pd.read_csv(self.scraped_csv)\n",
    "        return pd.DataFrame(columns=cols)\n",
    "    \n",
    "\n",
    "    def _append_to_csv(self, df: pd.DataFrame) -> None:\n",
    "        \"\"\"\n",
    "        Append the newly scraped lease data to the CSV file.\n",
    "\n",
    "        Args:\n",
    "            df (pd.DataFrame): The DataFrame with new lease data.\n",
    "        \"\"\"\n",
    "        if os.path.exists(self.scraped_csv):\n",
    "            df.to_csv(self.scraped_csv, mode='a', header=False, index=False)\n",
    "        else:\n",
    "            df.to_csv(self.scraped_csv, index=False)\n",
    "\n",
    "\n",
    "    def _quit(self) -> None:\n",
    "        \"\"\"\n",
    "        Close and quit the WebDriver.\n",
    "        \"\"\"\n",
    "        if self.driver is not None:\n",
    "            self.driver.close()\n",
    "            self.driver.quit()\n",
    "            self.driver = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Well Header Data from CC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatting_WellHeader_CC():\n",
    "    \n",
    "    # Reading well header csv to pandas DataFrame\n",
    "    df_wellheader_raw = pd.read_csv('well_header.csv',low_memory=False)\n",
    "\n",
    "    # Cleaning up the column names\n",
    "    df_wellheader_modified = df_wellheader_raw.copy() # Copying the DataFrame\n",
    "    df_wellheader_modified.columns = df_wellheader_modified.columns.str.lower().str.replace(' ', '_')  # Convert to lowercase and replace spaces with underscores\n",
    "\n",
    "    # Only grab Lease Nos. that are complete\n",
    "    df_wellHeader_complete_LeaseNo = df_wellheader_modified[df_wellheader_modified['lease_number'].apply(lambda x: len(str(x)) if pd.notnull(x) else 0) == 11].reset_index(drop=True)\n",
    "\n",
    "    # Returning unique LeaseNo. from well header\n",
    "    return pd.DataFrame(df_wellHeader_complete_LeaseNo['lease_number'].unique(), columns=['lease_number'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the Scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to handle scraping outside of the class\n",
    "def scrape_leases(csv_dir: str, leases_df: pd.DataFrame, limit: int = 1000) -> None:\n",
    "    \"\"\"\n",
    "    Scrapes leases from a DataFrame and stops after scraping 1,000 rows, with pauses every 25 rows. \n",
    "    Closes and re-opens the browser after every 5 leases.\n",
    "    \n",
    "    If the page shows 'Cookies are required for this application', it reloads the page and resumes scraping.\n",
    "    \n",
    "    Args:\n",
    "        csv_dir (str): The directory path where the scraped CSV file will be saved.\n",
    "        leases_df (pd.DataFrame): The DataFrame containing lease_number columns.\n",
    "        limit (int): The maximum number of leases to scrape in one session. Defaults to 1,000.\n",
    "    \"\"\"\n",
    "    logger.info(f\"\\n\\n################## Started Lease Scraping with {limit} limit. ##################\\n\\n\")\n",
    "\n",
    "    # Initialize the scraper with the provided CSV directory\n",
    "    logger.info(\"Initializing the scraper with provided CSV directory.\")\n",
    "    scraper = _LeaseDropNaturalGas_WebScraper(csv_dir=csv_dir)\n",
    "\n",
    "    # Read existing scraped leases from the CSV file\n",
    "    scraped_df = scraper._read_scraped_csv()\n",
    "    logger.info(f\"Read {len(scraped_df['lease_number'].unique()):,} previously unique scraped leases from {scraper.scraped_csv}.\")\n",
    "\n",
    "    # Filter out leases that have already been scraped\n",
    "    leases_to_scrape = leases_df[~(leases_df['lease_number'].isin(scraped_df['lease_number'].unique()))]\n",
    "    logger.info(f\"Found {len(leases_to_scrape):,} leases to scrape.\")\n",
    "\n",
    "    scraped_data: List[pd.DataFrame] = []\n",
    "    lease_not_found: List = []\n",
    "\n",
    "    try:\n",
    "        scraper._load_page()\n",
    "        logger.info(\"Loaded initial web page for scraping.\")\n",
    "        \n",
    "        for count, lease_no in enumerate(leases_to_scrape['lease_number'], start=1):\n",
    "            retry_count = 0\n",
    "            max_retries = 3  # Set the maximum number of retries\n",
    "\n",
    "            while retry_count < max_retries:\n",
    "                # Check if the page requires cookies before scraping the lease number\n",
    "                soup = BeautifulSoup(scraper.driver.page_source, 'html.parser')\n",
    "                h1_text = soup.find('h1').get_text(strip=True)\n",
    "                \n",
    "                if h1_text == 'Cookies are required for this application.':\n",
    "                    logger.info(f\"Cookies required error. Reloading page for lease {lease_no}. Retry {retry_count + 1}/{max_retries}.\")\n",
    "                    scraper._load_page()  # Reload the page\n",
    "                    retry_count += 1  # Increment the retry count\n",
    "                    continue  # Retry the same lease_no after reloading the page\n",
    "                \n",
    "                # Scrape the lease number if cookies are not required\n",
    "                logger.info(f\"Scraping data for lease number {lease_no}.\")\n",
    "                html_content: Optional[str] = scraper._get_NGL_Inquiry_html(lease_no, beg_dt='2201', end_dt='2410')\n",
    "\n",
    "                if html_content:\n",
    "                    df: pd.DataFrame = scraper._parse_html(html_content)\n",
    "                    df.insert(0, 'lease_number', lease_no)  # Create a new column with the lease_number and insert it at the beginning\n",
    "                    scraped_data.append(df)\n",
    "                    break  # Exit the retry loop if the scraping is successful\n",
    "                else:\n",
    "                    lease_not_found.append(lease_no)\n",
    "                    break  # Exit the retry loop if the lease is not found\n",
    "\n",
    "            scraper._clear_entry_labels()\n",
    "\n",
    "            time.sleep(1)\n",
    "\n",
    "            if count % 5 == 0:  # Close and reopen the browser after scraping every 5 leases\n",
    "                logger.info(f\"Closing and reopening browser after scraping {count} leases.\")\n",
    "                scraper._quit()\n",
    "                time.sleep(1)  # Wait for a short period before reopening the browser\n",
    "                logger.info(f\"Reopening the page to continue scraping after lease {count}.\")\n",
    "                scraper = _LeaseDropNaturalGas_WebScraper(csv_dir=csv_dir)\n",
    "                scraper._load_page()\n",
    "\n",
    "            # Pausing afet every 25 leases scraped\n",
    "            if count % 25 == 0:\n",
    "                logger.info(f\"Pausing for 30 seconds after scraping {count} leases.\")\n",
    "                time.sleep(30)\n",
    "\n",
    "            # Stopping after scraping the limit\n",
    "            if count >= limit:\n",
    "                logger.info(f\"Scraping limit of {limit} reached. Stopping.\")\n",
    "                break\n",
    "    finally:\n",
    "        scraper._quit()\n",
    "\n",
    "    # Save scraped data to CSV\n",
    "    if scraped_data:\n",
    "        full_df = pd.concat(scraped_data, ignore_index=True)\n",
    "        scraper._append_to_csv(full_df)\n",
    "        logger.info(f\"Scraped {count - len(lease_not_found)} leases and saved to {scraper.scraped_csv}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[scraper_ipynb] INFO (10-13 05:54 PM): \n",
      "\n",
      "################## Started Lease Scraping with 10 limit. ##################\n",
      "\n",
      " (Line: 14) [3313725787.py]\n",
      "[scraper_ipynb] INFO (10-13 05:54 PM): Initializing the scraper with provided CSV directory. (Line: 17) [3313725787.py]\n",
      "[scraper_ipynb] INFO (10-13 05:54 PM): Read 37 previously unique scraped leases from C:\\Users\\Apoorva.Saxena\\OneDrive - Sitio Royalties\\Desktop\\Project - Apoorva\\Python\\Scraping\\Texas-Comptroller-of-Public-Accounts-Scraper\\scraped_leases.csv. (Line: 22) [3313725787.py]\n",
      "[scraper_ipynb] INFO (10-13 05:54 PM): Found 10,618 leases to scrape. (Line: 26) [3313725787.py]\n",
      "[scraper_ipynb] INFO (10-13 05:54 PM): Loaded initial web page for scraping. (Line: 33) [3313725787.py]\n",
      "[scraper_ipynb] INFO (10-13 05:54 PM): Scraping data for lease number 08-P15667-O. (Line: 51) [3313725787.py]\n",
      "[scraper_ipynb] WARNING (10-13 05:54 PM): Lease table not found for lease 08-P15667-O on attempt 1. Retrying... (Line: 142) [4263674244.py]\n",
      "[scraper_ipynb] WARNING (10-13 05:54 PM): Lease table not found for lease 08-P15667-O on attempt 2. Retrying... (Line: 142) [4263674244.py]\n",
      "[scraper_ipynb] WARNING (10-13 05:54 PM): Lease table not found for lease 08-P15667-O on attempt 3. Retrying... (Line: 142) [4263674244.py]\n",
      "[scraper_ipynb] ERROR (10-13 05:55 PM): Failed to scrape lease 08-P15667-O after 3 attempts. (Line: 150) [4263674244.py]\n",
      "[scraper_ipynb] INFO (10-13 05:55 PM): Scraping data for lease number 08-055692-O. (Line: 51) [3313725787.py]\n",
      "[scraper_ipynb] WARNING (10-13 05:55 PM): Lease table not found for lease 08-055692-O on attempt 1. Retrying... (Line: 142) [4263674244.py]\n",
      "[scraper_ipynb] WARNING (10-13 05:55 PM): Lease table not found for lease 08-055692-O on attempt 2. Retrying... (Line: 142) [4263674244.py]\n",
      "[scraper_ipynb] WARNING (10-13 05:55 PM): Lease table not found for lease 08-055692-O on attempt 3. Retrying... (Line: 142) [4263674244.py]\n",
      "[scraper_ipynb] ERROR (10-13 05:55 PM): Failed to scrape lease 08-055692-O after 3 attempts. (Line: 150) [4263674244.py]\n",
      "[scraper_ipynb] INFO (10-13 05:55 PM): Scraping data for lease number 08-057309-O. (Line: 51) [3313725787.py]\n",
      "[scraper_ipynb] WARNING (10-13 05:55 PM): Lease table not found for lease 08-057309-O on attempt 1. Retrying... (Line: 142) [4263674244.py]\n",
      "[scraper_ipynb] WARNING (10-13 05:56 PM): Lease table not found for lease 08-057309-O on attempt 2. Retrying... (Line: 142) [4263674244.py]\n",
      "[scraper_ipynb] WARNING (10-13 05:56 PM): Lease table not found for lease 08-057309-O on attempt 3. Retrying... (Line: 142) [4263674244.py]\n",
      "[scraper_ipynb] ERROR (10-13 05:56 PM): Failed to scrape lease 08-057309-O after 3 attempts. (Line: 150) [4263674244.py]\n",
      "[scraper_ipynb] INFO (10-13 05:56 PM): Scraping data for lease number 08-291668-G. (Line: 51) [3313725787.py]\n",
      "[scraper_ipynb] INFO (10-13 05:56 PM): Successfully scraped lease 08-291668-G (Attempt 1) (Line: 138) [4263674244.py]\n",
      "[scraper_ipynb] INFO (10-13 05:56 PM): Scraping data for lease number 7C-021352-O. (Line: 51) [3313725787.py]\n",
      "[scraper_ipynb] INFO (10-13 05:57 PM): Successfully scraped lease 7C-021352-O (Attempt 1) (Line: 138) [4263674244.py]\n",
      "[scraper_ipynb] INFO (10-13 05:57 PM): Closing and reopening browser after scraping 5 leases. (Line: 68) [3313725787.py]\n",
      "[scraper_ipynb] INFO (10-13 05:57 PM): Reopening the page to continue scraping after lease 5. (Line: 71) [3313725787.py]\n",
      "[scraper_ipynb] INFO (10-13 05:57 PM): Scraping data for lease number 7C-021465-O. (Line: 51) [3313725787.py]\n",
      "[scraper_ipynb] INFO (10-13 05:57 PM): Successfully scraped lease 7C-021465-O (Attempt 1) (Line: 138) [4263674244.py]\n",
      "[scraper_ipynb] INFO (10-13 05:57 PM): Scraping data for lease number 08-057308-O. (Line: 51) [3313725787.py]\n",
      "[scraper_ipynb] WARNING (10-13 05:58 PM): Lease table not found for lease 08-057308-O on attempt 1. Retrying... (Line: 142) [4263674244.py]\n",
      "[scraper_ipynb] WARNING (10-13 05:58 PM): Lease table not found for lease 08-057308-O on attempt 2. Retrying... (Line: 142) [4263674244.py]\n",
      "[scraper_ipynb] WARNING (10-13 05:58 PM): Lease table not found for lease 08-057308-O on attempt 3. Retrying... (Line: 142) [4263674244.py]\n",
      "[scraper_ipynb] ERROR (10-13 05:58 PM): Failed to scrape lease 08-057308-O after 3 attempts. (Line: 150) [4263674244.py]\n",
      "[scraper_ipynb] INFO (10-13 05:58 PM): Scraping data for lease number 08-292158-G. (Line: 51) [3313725787.py]\n",
      "[scraper_ipynb] INFO (10-13 05:58 PM): Successfully scraped lease 08-292158-G (Attempt 1) (Line: 138) [4263674244.py]\n",
      "[scraper_ipynb] INFO (10-13 05:58 PM): Scraping data for lease number 08-056336-O. (Line: 51) [3313725787.py]\n",
      "[scraper_ipynb] INFO (10-13 05:58 PM): Successfully scraped lease 08-056336-O (Attempt 1) (Line: 138) [4263674244.py]\n",
      "[scraper_ipynb] INFO (10-13 05:58 PM): Scraping data for lease number 08-058562-O. (Line: 51) [3313725787.py]\n",
      "[scraper_ipynb] INFO (10-13 05:58 PM): Successfully scraped lease 08-058562-O (Attempt 1) (Line: 138) [4263674244.py]\n",
      "[scraper_ipynb] INFO (10-13 05:58 PM): Closing and reopening browser after scraping 10 leases. (Line: 68) [3313725787.py]\n",
      "[scraper_ipynb] INFO (10-13 05:59 PM): Reopening the page to continue scraping after lease 10. (Line: 71) [3313725787.py]\n",
      "[scraper_ipynb] INFO (10-13 05:59 PM): Scraping limit of 10 reached. Stopping. (Line: 82) [3313725787.py]\n",
      "[scraper_ipynb] INFO (10-13 05:59 PM): Scraped 6 leases and saved to C:\\Users\\Apoorva.Saxena\\OneDrive - Sitio Royalties\\Desktop\\Project - Apoorva\\Python\\Scraping\\Texas-Comptroller-of-Public-Accounts-Scraper\\scraped_leases.csv. (Line: 91) [3313725787.py]\n"
     ]
    }
   ],
   "source": [
    "scrape_leases(csv_dir=r\"C:\\Users\\Apoorva.Saxena\\OneDrive - Sitio Royalties\\Desktop\\Project - Apoorva\\Python\\Scraping\\Texas-Comptroller-of-Public-Accounts-Scraper\", \n",
    "              leases_df = formatting_WellHeader_CC(), limit=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the scraper\n",
    "\n",
    "# scraper = _LeaseDropNaturalGas_WebScraper(\n",
    "#     csv_dir=r\"C:\\Users\\Apoorva.Saxena\\OneDrive - Sitio Royalties\\Desktop\\Project - Apoorva\\Python\\Scraping\\Texas-Comptroller-of-Public-Accounts-Scraper\"\n",
    "#     )\n",
    "\n",
    "# try:\n",
    "#     # Fill the form and get the HTML content\n",
    "#     html_content = scraper._get_NGL_Inquiry_html(lease_no='7C-017147-O', beg_dt='2301', end_dt='2410')\n",
    "\n",
    "#     # Parse the HTML and get the cleaned DataFrame\n",
    "#     if html_content:\n",
    "#         df = scraper._parse_html(html=html_content)\n",
    "# finally:\n",
    "#     scraper._quit()\n",
    "\n",
    "# str(date.today().year)[2:] + str(date.today().month).zfill(2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
